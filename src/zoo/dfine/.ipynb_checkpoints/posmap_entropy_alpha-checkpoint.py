"""
ä½ç½®å…ˆéªŒï¼ˆPosMapï¼‰ + ç†µå¢å¼ºæ¨¡å—ï¼ˆæ— è’¸é¦ç‰ˆï¼‰
æ ¸å¿ƒè®¾è®¡æ€æƒ³ï¼š
  1) ç†µå›¾ Entropyï¼šä»…ç”±ç‰¹å¾è®¡ç®—ï¼Œç²—ç²’åº¦å®šä½â€œå€¼å¾—å¢å¼ºçš„åŒºåŸŸâ€
  2) å­¦ç”Ÿä½ç½®å›¾ PosMapï¼šå‘Šè¯‰ç½‘ç»œå“ªé‡Œæ›´å¯èƒ½æ˜¯ç›®æ ‡åŒºåŸŸ
  3) è‡ªé€‚åº”åŒé—¨æ§ Alphaï¼š
       - é€šé“é—¨ alpha_cï¼šé€‰æ‹©å“ªäº›é€šé“éœ€è¦å¢å¼ºï¼ˆç±»/è¯­ä¹‰ç›¸å…³ï¼‰
       - ç©ºé—´é—¨ alpha_sï¼šåœ¨ç†µé«˜çš„åŒºåŸŸé‡Œï¼Œç­›å‡ºçœŸæ­£çš„ç›®æ ‡åƒç´ 
     Alpha åªä¾èµ– feature + posmapï¼Œä¸ä¾èµ– entropyï¼Œæœ¬è´¨æ˜¯ä¸€ä¸ªâ€œç›®æ ‡æ„ŸçŸ¥çš„æ³¨æ„åŠ›å›¾â€
  4) æœ€ç»ˆå¢å¼ºï¼ˆåœ¨ FPN / PAN ä¸­ä½¿ç”¨ï¼‰ï¼š
       fused_enh = fused * (1 + entropy_map * alpha)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .utils import get_activation


# ================================================================
# 1. çº¯ç†µå›¾ OverlapEntropyï¼ˆä¸å†æ³¨å…¥ä½ç½®å…ˆéªŒï¼‰
# ================================================================
class OverlapEntropyWithPos(nn.Module):
    """
    è™½ç„¶åå­—é‡Œå¸¦ WithPosï¼Œä¸ºäº†å…¼å®¹ HybridEncoder ä¸æ”¹ç±»åï¼Œ
    ä½†è¿™é‡Œå·²ç»ä¸å†ä½¿ç”¨ pos_mapï¼Œç†µå›¾åªç”±ç‰¹å¾ x è®¡ç®—ã€‚
    """
    def __init__(self, kernel_size=3, stride=1, eps=1e-8, num_scales=3):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.eps = eps
        self.unfold = nn.Unfold(
            kernel_size=kernel_size,
            stride=stride,
            padding=kernel_size // 2
        )

    def forward(self, x, pos_map=None, scale_idx=0):
        """
        x: [B, C, H, W]
        è¿”å›ï¼šentropy_map [B, 1, H, W]ï¼Œå½’ä¸€åŒ–åˆ° [0,1]
        """
        B, C, H, W = x.shape

        # unfold â†’ å±€éƒ¨ patch
        patches = self.unfold(x)                   # [B, C*K, L]
        K = self.kernel_size * self.kernel_size
        patches = patches.view(B, C, K, -1)        # [B, C, K, L]

        # patch å†… softmax å½“ä½œæ¦‚ç‡
        p = F.softmax(patches, dim=2) + self.eps   # [B, C, K, L]
        entropy = -torch.sum(p * torch.log(p), dim=2)   # [B, C, L]
        entropy = entropy.mean(dim=1)                    # [B, L]
        entropy = entropy.view(B, 1, H, W)               # [B, 1, H, W]

        # batch å†…å½’ä¸€åŒ– â†’ [0,1]
        with torch.no_grad():
            emin = entropy.amin(dim=(1, 2, 3), keepdim=True)
            emax = entropy.amax(dim=(1, 2, 3), keepdim=True)
        entropy = (entropy - emin) / (emax - emin + self.eps)

        # ä¸å†åŠ å…¥ä»»ä½• pos_map
        return entropy


# ================================================================
# 2. è‡ªé€‚åº” Alphaï¼ˆé€šé“ + ç©ºé—´ï¼‰ï¼Œåªçœ‹ feat + posmap
# ================================================================
class AdaptiveAlphaWithPos(nn.Module):
    """
    Alpha çš„å®šä½ä½œç”¨ï¼š
      - é€šé“é—¨ alpha_cï¼šå‘Šè¯‰â€œå¢å¼ºå“ªäº›è¯­ä¹‰é€šé“â€
      - ç©ºé—´é—¨ alpha_sï¼šåœ¨ç²—ç²’åº¦ entropy åŒºåŸŸä¸­ï¼ŒæŒ‘å‡ºæ›´åƒç›®æ ‡çš„åƒç´ 
    Alpha ä¸ä½¿ç”¨ entropyï¼Œåªä½¿ç”¨ feature + posmapã€‚
    æœ€ç»ˆåœ¨ HybridEncoder ä¸­ä½¿ç”¨ï¼š fused_enh = fused * (1 + entropy * alpha)
    """
    def __init__(self, channels, reduction=16):
        super().__init__()
        hidden = max(4, channels // reduction)

        # ---------- é€šé“é—¨ï¼šå…¨å±€æ± åŒ– + MLP ----------
        self.channel_fc = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, hidden, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden, channels, 1, bias=True),
            nn.Sigmoid()  # [B, C, 1, 1]
        )

        # ---------- ç©ºé—´é—¨ï¼šposmap å¼•å¯¼ + feature ç»†åŒ– ----------
        # posmap å…ˆåšä¸€ä¸ªå° convï¼Œå¾—åˆ°â€œç›®æ ‡æ³¨æ„åŠ›å›¾â€
        self.pos_conv = nn.Sequential(
            nn.Conv2d(1, hidden, 3, padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden, 1, 1, bias=True),
            nn.Sigmoid()   # [B, 1, H, W]
        )

        # å†å°† feature + pos_attn èåˆï¼Œç”Ÿæˆæœ€ç»ˆçš„ spatial gate
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(channels + 1, hidden, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden, 1, 1, bias=True),
            nn.Sigmoid()   # [B, 1, H, W]
        )

        # å•å°ºåº¦å¢å¼ºåˆ†æ”¯ï¼šç»“æ„ç®€å•ä¸€ç‚¹ï¼Œç”¨äº pre-fusion
        self.single_scale_conv = nn.Sequential(
            nn.Conv2d(channels + 1, hidden, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden, channels, 1, bias=True),
            nn.Sigmoid()   # [B, C, H, W]
        )

    # -----------------------------
    # èåˆå‰ï¼šå•å°ºåº¦å¢å¼ºï¼ˆåªçœ‹ç‰¹å¾ + posmapï¼‰
    # -----------------------------
    def forward_single_scale(self, x, pos_map):
        """
        x: [B, C, H, W]
        pos_map: [B, 1, H', W'] or None
        """
        if pos_map is None:
            return x
        pos_map_resized = F.interpolate(
            pos_map, size=x.shape[2:], mode="bilinear", align_corners=False
        )
        alpha_single = self.single_scale_conv(
            torch.cat([x, pos_map_resized], dim=1)
        )  # [B, C, H, W]
        # è¿™é‡Œåªåšè½»å¾®å¢å¼ºï¼Œé¿å…ç ´å backbone è¯­ä¹‰
        return x * (1.0 + 0.2 * alpha_single)

    # -----------------------------
    # èåˆæ—¶ï¼šåŒé—¨æ§ï¼ˆé€šé“ + ç©ºé—´ï¼‰ï¼Œåªç”¨ feat + posmap
    # -----------------------------
    def forward_fusion(self, x, entropy_map, pos_map):
        """
        x: [B, C, H, W]
        entropy_map: [B, 1, H, W]ï¼ˆè¿™é‡Œä¸å‚ä¸ alpha è®¡ç®—ï¼Œåªåœ¨å¤–éƒ¨ç”¨äºä¹˜ï¼‰
        pos_map: [B, 1, H', W'] or None
        è¿”å›ï¼šalpha [B, 1, H, W]ï¼Œåœ¨å¤–éƒ¨ä¸ entropy ç›¸ä¹˜ä½¿ç”¨
        """
        B, C, H, W = x.shape

        # é€šé“é—¨ï¼šä¸ç†µæ— å…³ï¼Œä¸»è¦çœ‹ç±»åˆ«/è¯­ä¹‰
        alpha_c = self.channel_fc(x)  # [B, C, 1, 1]

        # ç©ºé—´é—¨ï¼šç”± posmap æŒ‡å¼•
        if pos_map is None:
            # æ²¡æœ‰ posmap çš„æƒ…å†µä¸‹ï¼Œé€€åŒ–ä¸ºâ€œé€šé“æ³¨æ„åŠ› + feature ç©ºé—´æ³¨æ„åŠ›â€
            dummy_pos = torch.zeros(B, 1, H, W, device=x.device, dtype=x.dtype)
            pos_attn = self.pos_conv(dummy_pos)  # [B,1,H,W]
        else:
            pos_resized = F.interpolate(
                pos_map, size=(H, W), mode="bilinear", align_corners=False
            )
            pos_attn = self.pos_conv(pos_resized)  # [B,1,H,W]

        # ç‰¹å¾ + pos_attn â†’ ç©ºé—´ gate
        spatial_input = torch.cat([x, pos_attn], dim=1)   # [B, C+1, H, W]
        alpha_s = self.spatial_conv(spatial_input)        # [B, 1, H, W]

        # å°†é€šé“é—¨å¹¿æ’­åˆ°ç©ºé—´ï¼š [B,C,1,1] â†’ [B,C,H,W]
        alpha_c_map = alpha_c.expand(-1, -1, H, W)        # [B, C, H, W]
        # å†å‹æˆ 1 é€šé“ï¼ˆå–å¹³å‡ï¼‰ï¼Œå¾—åˆ° [B,1,H,W] çš„é€šé“æ³¨æ„åŠ›
        alpha_c_spatial = alpha_c_map.mean(dim=1, keepdim=True)  # [B,1,H,W]

        # æœ€ç»ˆ alphaï¼šé€šé“æ„ŸçŸ¥ Ã— ç©ºé—´æ„ŸçŸ¥
        alpha = alpha_c_spatial * alpha_s  # [B,1,H,W]

        return alpha  # åç»­åœ¨ HybridEncoder ä¸­ä¸ entropy_map ç›¸ä¹˜ä½¿ç”¨

        
    def forward(self, x, entropy_map=None, pos_map=None):
        return self.forward_fusion(x, entropy_map, pos_map)

# ================================================================
# 3. å­¦ç”Ÿä½ç½®å…ˆéªŒï¼ˆä¸»å¹²ä¸ä¹‹å‰ç‰ˆæœ¬å…¼å®¹ï¼Œåªæ˜¯ä¾› alpha å¼•å¯¼ï¼‰
# ================================================================
class StudentPosHead(nn.Module):
    """
    è¾“å‡ºï¼šæ¯ä¸ª scale çš„æ¦‚ç‡ä½ç½®å›¾ [B,1,H,W]
    ç›‘ç£ï¼šcenter + ringï¼ˆç”± compute_label_loss ä½¿ç”¨ï¼‰
    """
    def __init__(self, feat_channels_list, mid_scale_idx=1, mid_stride=16):
        super().__init__()
        self.mid_scale_idx = mid_scale_idx
        self.mid_stride = mid_stride
        self.num_scales = len(feat_channels_list)

        mid_ch = feat_channels_list[mid_scale_idx]
        self.mid_base = nn.Sequential(
            nn.Conv2d(mid_ch, mid_ch // 4, 3, padding=1, bias=False),
            nn.BatchNorm2d(mid_ch // 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_ch // 4, 1, 1)
        )

        self.scale_refine = nn.ModuleList()
        for ch in feat_channels_list:
            self.scale_refine.append(
                nn.Sequential(
                    nn.Conv2d(ch + 1, ch // 8, 1, bias=False),
                    nn.BatchNorm2d(ch // 8),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(ch // 8, 1, 3, padding=1),
                )
            )

        self.sigmoid = nn.Sigmoid()

    # ============================================================
    # å†…éƒ¨ï¼šç”Ÿæˆä¸­å¿ƒ + ring é«˜æ–¯å›¾ï¼Œç”¨äºæ ‡ç­¾ç›‘ç£ï¼ˆéå¿…é¡»ç”¨äºå¢å¼ºï¼‰
    # ============================================================
    def _make_gaussian_and_ring(self, H, W, boxes, stride, device):
        ys = torch.arange(H, device=device).view(H, 1)
        xs = torch.arange(W, device=device).view(1, W)

        center_map = torch.zeros(1, H, W, device=device)
        ring_map = torch.zeros(1, H, W, device=device)

        for box in boxes:
            if box.sum() < 1e-6:
                continue

            # è½¬åˆ° feature map åæ ‡ï¼ˆä½ çš„ gt_boxes æ˜¯ cx,cy,w,h åƒç´ ï¼‰
            cx, cy, bw, bh = box
            x1 = (cx - bw / 2) / stride
            x2 = (cx + bw / 2) / stride
            y1 = (cy - bh / 2) / stride
            y2 = (cy + bh / 2) / stride

            x1 = x1.clamp(0, W - 1)
            x2 = x2.clamp(0, W - 1)
            y1 = y1.clamp(0, H - 1)
            y2 = y2.clamp(0, H - 1)

            cx_f = (x1 + x2) / 2
            cy_f = (y1 + y2) / 2
            w = (x2 - x1 + 1).clamp(min=2.0)
            h = (y2 - y1 + 1).clamp(min=2.0)

            area = w * h

            # å°ç›®æ ‡ï¼šåªç”¨ä¸­å¿ƒé«˜æ–¯
            if area < 20 * 20:
                sigma_x = w
                sigma_y = h
                g = torch.exp(-(((xs - cx_f) ** 2) / (2 * sigma_x ** 2) +
                                ((ys - cy_f) ** 2) / (2 * sigma_y ** 2)))
                center_map = torch.max(center_map, g)
                continue

            # å¤§ç›®æ ‡ï¼šä¸­å¿ƒ + ç¯å½¢ï¼ˆè½®å»“é™„è¿‘ï¼‰
            sigma_x = w
            sigma_y = h
            inner = torch.exp(-(((xs - cx_f) ** 2) / (2 * sigma_x ** 2) +
                                ((ys - cy_f) ** 2) / (2 * sigma_y ** 2)))
            outer = torch.exp(-(((xs - cx_f) ** 2) / (2 * (1.8 * sigma_x) ** 2) +
                                ((ys - cy_f) ** 2) / (2 * (1.8 * sigma_y) ** 2)))
            ring = (outer - inner).clamp(0, 1)

            center_map = torch.max(center_map, inner)
            ring_map = torch.max(ring_map, ring)

        return center_map, ring_map

    # ============================================================
    # ğŸ”· GT supervision: ç”Ÿæˆä¸­å¿ƒå›¾ + è¾¹ç¼˜ç¯å›¾ï¼ˆä¾› loss ç”¨ï¼‰
    # ============================================================
    def generate_gt_pos_maps(self, gt_boxes, feats_list):
        """
        gt_boxes: List[Tensor], æ¯ä¸ª [Ni,4]ï¼Œåƒç´ åæ ‡ cx,cy,w,h
        feats_list: å¤šå°ºåº¦ç‰¹å¾
        è¿”å›ï¼šList[Tensor]ï¼Œæ¯ä¸ªå°ºåº¦ [B,2,H,W]ï¼ˆ0: center, 1: ringï¼‰
        """
        gt_maps = []
        device = feats_list[0].device

        for s in range(self.num_scales):
            feat = feats_list[s]
            B, _, H, W = feat.shape
            stride = self.mid_stride // (2 ** (self.mid_scale_idx - s))

            maps = torch.zeros(B, 2, H, W, device=device)

            for b in range(B):
                if gt_boxes[b] is None or gt_boxes[b].numel() == 0:
                    continue
                c_map, r_map = self._make_gaussian_and_ring(
                    H, W, gt_boxes[b], stride, device
                )
                maps[b, 0] = c_map
                maps[b, 1] = r_map

            gt_maps.append(maps.clamp(0, 1))

        return gt_maps

    # ============================================================
    # ğŸ”· Forward: ç”Ÿæˆå­¦ç”Ÿä½ç½®å›¾ï¼ˆä¾› alpha ä½¿ç”¨ï¼‰
    # ============================================================
    def forward(self, feats_list):
        """
        feats_list: List[Tensor] å¤šå°ºåº¦ç‰¹å¾
        è¿”å›ï¼š List[Tensor]ï¼Œæ¯ä¸ª [B,1,H,W]ï¼ˆæ¦‚ç‡å›¾ï¼‰
        """
        mid_feat = feats_list[self.mid_scale_idx]
        mid_pos = self.mid_base(mid_feat)  # [B,1,H_mid,W_mid]

        pos_maps = []
        for s, feat in enumerate(feats_list):
            H, W = feat.shape[2:]
            pos_interp = F.interpolate(
                mid_pos, size=(H, W), mode="bilinear", align_corners=False
            )
            refine = self.scale_refine[s](torch.cat([feat, pos_interp], dim=1))
            pos = 0.7 * pos_interp + 0.3 * refine
            pos_maps.append(self.sigmoid(pos))  # [B,1,H,W]
        return pos_maps


# ================================================================
# 4. æ ‡ç­¾ç›‘ç£æŸå¤±ï¼ˆä¸­å¿ƒ + ç¯å½¢è¾¹ç¼˜ï¼‰
# ================================================================
def compute_label_loss(student_maps, gt_maps, scale_weights):
    """
    student_maps: List[Tensor], æ¯ä¸ª [B,1,H,W]
    gt_maps:      List[Tensor], æ¯ä¸ª [B,2,H,W] (center, ring)
    scale_weights: List[float]ï¼Œæ¯ä¸ªå°ºåº¦çš„ loss æƒé‡ï¼Œä¾‹å¦‚ [0.25, 0.5, 0.25]
    """
    device = student_maps[0].device
    total = torch.tensor(0.0, device=device)
    eps = 1e-6

    # è½¬æˆ tensor æ–¹ä¾¿åœ¨ GPU ä¸Šåš
    scale_weights_t = torch.tensor(
        scale_weights, device=device, dtype=torch.float32
    )

    for s_idx, (s_map, g_map) in enumerate(zip(student_maps, gt_maps)):
        if g_map is None:
            continue

        center = g_map[:, 0:1]  # [B,1,H,W]
        ring = g_map[:, 1:2]    # [B,1,H,W]

        s = s_map.clamp(eps, 1 - eps)

        # BCE for center
        logits = torch.logit(s, eps=eps)
        bce_center = F.binary_cross_entropy_with_logits(
            logits.float(), center.float()
        )

        # L1 for ring
        ring_loss = F.l1_loss(s.float(), ring.float())

        w = scale_weights_t[s_idx]
        total = total + w * (bce_center + 0.5 * ring_loss)

    return total
